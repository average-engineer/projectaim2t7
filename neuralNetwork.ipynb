{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from scipy.ndimage.filters import uniform_filter1d\n",
    "from scipy.signal import butter , filtfilt\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import signal\n",
    "\n",
    "import pylab\n",
    "import os.path\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folders = glob.glob(r'./Data/Smartphone3/*')\n",
    "#print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data and initialising the used arrays for the data and the label\n",
    "\n",
    "# Shapes of arrays\n",
    "# Training input:  (750, 40000,4)\n",
    "# Training label:  (750,)\n",
    "# Subject label:   (750,)\n",
    "training_data=np.zeros((750,60000,4))\n",
    "training_label=np.zeros((750))\n",
    "training_subject=np.zeros((750))\n",
    "training_data_len=np.zeros((750))\n",
    "training_data_freq=np.zeros((750))\n",
    "k=0\n",
    "\n",
    "s_id = range(150, 275) \n",
    "\n",
    "for i in s_id:\n",
    "    #if i != 219:\n",
    "    for j in range(2):\n",
    "        \n",
    "        path_norm=\"./Data/Smartphone3/subject\"+str(i).zfill(2)+\"_normal0\"+str(j+1)+\"/Accelerometer.csv\"\n",
    "        path_up=\"./Data/Smartphone3/subject\"+str(i).zfill(2)+\"_upstairs0\"+str(j+1)+\"/Accelerometer.csv\"\n",
    "        path_down=\"./Data/Smartphone3/subject\"+str(i).zfill(2)+\"_downstairs0\"+str(j+1)+\"/Accelerometer.csv\"\n",
    "\n",
    "        # print(path_norm, path_up, path_down)\n",
    "        \n",
    "        if os.path.isfile(path_norm): \n",
    "            \n",
    "            data_norm=pd.read_csv(path_norm,sep=\",\", header=None)\n",
    "\n",
    "            #print(data_norm[0])\n",
    "            training_data[k,0:(len(data_norm[0])-1),0]=data_norm[0][1:]\n",
    "            training_data[k,0:(len(data_norm[0])-1),1]=data_norm[1][1:]\n",
    "            training_data[k,0:(len(data_norm[0])-1),2]=data_norm[2][1:]\n",
    "            training_data[k,0:(len(data_norm[0])-1),3]=data_norm[3][1:]\n",
    "            training_data_len[k]=len(data_norm[0])\n",
    "            training_label[k]=0\n",
    "            training_subject[k]=i\n",
    "            # print(training_data[k,:])\n",
    "            k=k+1\n",
    "            \n",
    "\n",
    "                \n",
    "        if os.path.isfile(path_up): \n",
    "            \n",
    "            data_up=pd.read_csv(path_up,sep=\",\", header=None)\n",
    "\n",
    "            #print(data_up[0])\n",
    "            training_data[k,0:(len(data_up[0])-1),0]=data_up[0][1:]\n",
    "            training_data[k,0:(len(data_up[0])-1),1]=data_up[1][1:]\n",
    "            training_data[k,0:(len(data_up[0])-1),2]=data_up[2][1:]\n",
    "            training_data[k,0:(len(data_up[0])-1),3]=data_up[3][1:]\n",
    "            training_data_len[k]=len(data_up[0])\n",
    "            training_label[k]=1\n",
    "            training_subject[k]=i\n",
    "            # print(training_data[k,:])\n",
    "            k=k+1\n",
    "                \n",
    "        if os.path.isfile(path_down): \n",
    "            \n",
    "            data_down=pd.read_csv(path_down,sep=\",\", header=None)\n",
    "\n",
    "            #print(data_down[0])\n",
    "            training_data[k,0:(len(data_down[0])-1),0]=data_down[0][1:]\n",
    "            training_data[k,0:(len(data_down[0])-1),1]=data_down[1][1:]\n",
    "            training_data[k,0:(len(data_down[0])-1),2]=data_down[2][1:]\n",
    "            training_data[k,0:(len(data_down[0])-1),3]=data_down[3][1:]\n",
    "            training_data_len[k]=len(data_down[0])\n",
    "            training_label[k]=2\n",
    "            training_subject[k]=i\n",
    "            # print(training_data[k,:])\n",
    "            k=k+1\n",
    "                \n",
    "number_of_inputs=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the frequency of the sensors\n",
    "for i in range(number_of_inputs):\n",
    "     \n",
    "    #print(training_data[i,int(training_data_len[i]-1),0])\n",
    "    training_data_freq[i]=(training_data_len[i]-1)/training_data[i,round(training_data_len[i]-2),0]\n",
    "    print(training_data_freq[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter of the data\n",
    "# This first filter is used on the data\n",
    "\n",
    "\n",
    "cutFreq = 10\n",
    "training_data_filtered=np.zeros((750,60000,4))\n",
    "training_data_filtered[:,:,0]=training_data[:,:,0]\n",
    "\"A single Butterworth filter with a cutoff frequency between 0.1 and 15 Hz is \"\n",
    "\"sufficient to reduce a large component of noise from the data and serves \"\n",
    "\"as the best preprocessing step\"\n",
    "#     sampling_rate = round(3/(time[2]-time[1]),3)    # sampling rate 416 HZ \n",
    "for i in range(number_of_inputs):\n",
    "    for j in range(1,4):\n",
    "        sampling_rate=training_data_freq[i]\n",
    "        if sampling_rate >= 20:\n",
    "            cutoff_freq = cutFreq/(0.5*sampling_rate)  \n",
    "            # cutoff_freq = 5\n",
    "            b,a = butter(2, cutoff_freq, btype = 'low',analog= False)\n",
    "            data = training_data[i,:int(training_data_len[i]-2),j] #dataset.iloc[:,1:2]\n",
    "            data = np.transpose(data)\n",
    "            filtered_data = filtfilt(b,a,data)\n",
    "            training_data_filtered[i,:int(training_data_len[i]-2),j] = np.transpose(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'number_of_imputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7412/4036407563.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;34m\"as the best preprocessing step\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#     sampling_rate = round(3/(time[2]-time[1]),3)    # sampling rate 416 HZ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_imputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0msampling_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_data_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'number_of_imputs' is not defined"
     ]
    }
   ],
   "source": [
    "# This second filter is only used to find the extraction points on the data\n",
    "# With the low cut of frequency only a sinusoidal oscillation remains\n",
    "\n",
    "cutFreq = 1\n",
    "training_data_filtered_2=np.zeros((750,60000,4))\n",
    "training_data_filtered_2[:,:,0]=training_data[:,:,0]\n",
    "\"A single Butterworth filter with a cutoff frequency between 0.1 and 15 Hz is \"\n",
    "\"sufficient to reduce a large component of noise from the data and serves \"\n",
    "\"as the best preprocessing step\"\n",
    "#     sampling_rate = round(3/(time[2]-time[1]),3)    # sampling rate 416 HZ \n",
    "for i in range(number_of_imputs):\n",
    "    for j in range(1,4):\n",
    "        sampling_rate=training_data_freq[i]\n",
    "        cuttoff_freq = cutFreq/(0.5*sampling_rate)  \n",
    "        b,a = butter(2, cuttoff_freq, btype = 'low',analog= False)\n",
    "        data = training_data[i,:int(training_data_len[i]-1),j] #dataset.iloc[:,1:2]\n",
    "        data = np.transpose(data)\n",
    "        filtered_data = filtfilt(b,a,data)\n",
    "        training_data_filtered_2[i,:int(training_data_len[i]-1),j] = np.transpose(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot how the filtered data looks\n",
    "for i in range(2):\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "    plt.figure(i)\n",
    "    plt.plot(training_data[i,0:int(training_data_len[i]),0],training_data[i,0:int(training_data_len[i]),1])\n",
    "    plt.plot(training_data_filtered[i,0:int(training_data_len[i]),0],training_data_filtered[i,0:int(training_data_len[i]),1])\n",
    "    \n",
    "    print(training_label[i])\n",
    "    print(training_subject[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extraction\n",
    "#The finding of the relevant peaks of the sinusoidal oscillation is easy \n",
    "#and the distance between the peaks is used to find the walking sequence\n",
    "\n",
    "training_data_extracted=np.zeros((number_of_inputs,60000,4))\n",
    "training_data_len_extracted=np.zeros((number_of_inputs))\n",
    "peaks_i=np.zeros((number_of_inputs,700))\n",
    "\n",
    "extract_start=0\n",
    "extract_end=0\n",
    "safe_range_factor=7\n",
    "safe_range_factor_m=safe_range_factor-1\n",
    "# for i in range(1,30):\n",
    "for i in range(number_of_inputs):\n",
    "    \n",
    "    peak_indices, _ =find_peaks(training_data_filtered_2[i,0:int(training_data_len[i]),1], prominence=0.4)#,distance=150\n",
    "    peak_distance=np.zeros(len(peak_indices))\n",
    "    peaks_i[i,:len(peak_indices)]=peak_indices\n",
    "\n",
    "    if peak_indices.shape[0] > 1:\n",
    "\n",
    "    # print (type(peak_indices))\n",
    "    \n",
    "        for k in range(len(peak_indices)-1):\n",
    "            peak_distance[k]=peak_indices[k+1]-peak_indices[k]\n",
    "            \n",
    "        peak_distance_mean=np.mean(peak_distance[len(peak_distance)//safe_range_factor:safe_range_factor_m*len(peak_distance)//safe_range_factor])\n",
    "        try:\n",
    "            extract_start_peak=int(len(peak_distance)//safe_range_factor) - np.where(np.logical_or(np.flip(peak_distance[:len(peak_distance)//safe_range_factor]) > 1.2*peak_distance_mean , np.flip(peak_distance[:len(peak_distance)//safe_range_factor]) < 0.8*peak_distance_mean))[0][0]\n",
    "            extract_end_peak=int(safe_range_factor_m*len(peak_distance)//safe_range_factor) - 1 + np.where(np.logical_or( peak_distance[safe_range_factor_m*len(peak_distance)//safe_range_factor:len(peak_distance)]  > 1.2*peak_distance_mean, peak_distance[safe_range_factor_m*len(peak_distance)//safe_range_factor:len(peak_distance)]< 0.8*peak_distance_mean))[0][0]\n",
    "        except:\n",
    "            extract_start_peak=int(len(peak_distance)//safe_range_factor)\n",
    "            extract_end_peak=int(safe_range_factor_m*len(peak_distance)//safe_range_factor)\n",
    "            \n",
    "        extract_start=peak_indices[extract_start_peak]\n",
    "        extract_end=peak_indices[extract_end_peak]\n",
    "        \n",
    "        \n",
    "        training_data_len_extracted[i]=extract_end-extract_start\n",
    "        \n",
    "        for j in range(0,4):\n",
    "            training_data_extracted[i,:int(training_data_len_extracted[i]),j]=training_data_filtered[i,extract_start:extract_end,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot to show the extracted data\n",
    "\n",
    "for i in range(3):\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "    plt.figure(i)\n",
    "    plt.plot(training_data_filtered_2[i,0:int(training_data_len[i]),0],training_data_filtered_2[i,0:int(training_data_len[i]),1])\n",
    "    plt.plot(training_data_extracted[i,0:int(training_data_len_extracted[i]),0],training_data_extracted[i,0:int(training_data_len_extracted[i]),1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Rotation \n",
    "#The Rotation of the data is based of the gravitational acceleration.\n",
    "#This gets detected with the mean of the time series for every axis and the axis with the highest absolute value of the \n",
    "#mean gets set as the y-axis.\n",
    "#\n",
    "\n",
    "training_data_rotated=np.zeros((number_of_inputs,60000,4))\n",
    "\n",
    "for i in range(number_of_inputs):\n",
    "    \n",
    "# for i in range(1):\n",
    "    training_data_rotated[i,:int(training_data_len_extracted[i]),0]=training_data_extracted[i,:int(training_data_len_extracted[i]),0]\n",
    "    \n",
    "    #for j in range(1,4):\n",
    "    mean_x=np.abs(np.mean(training_data_extracted[i,:int(training_data_len_extracted[i]),1]))\n",
    "    mean_y=np.abs(np.mean(training_data_extracted[i,:int(training_data_len_extracted[i]),2]))\n",
    "    mean_z=np.abs(np.mean(training_data_extracted[i,:int(training_data_len_extracted[i]),3]))\n",
    "    \n",
    "    mean_y_sign=np.mean(training_data_extracted[i,:int(training_data_len_extracted[i]),2])\n",
    "    \n",
    "    index=np.argmax([mean_x,mean_y,mean_z])\n",
    "    if mean_y_sign>0:\n",
    "        if index==0:\n",
    "            training_data_rotated[i,:int(training_data_len_extracted[i]),2]=training_data_extracted[i,:int(training_data_len_extracted[i]),1]\n",
    "            training_data_rotated[i,:int(training_data_len_extracted[i]),1]=training_data_extracted[i,:int(training_data_len_extracted[i]),2]\n",
    "            training_data_rotated[i,:int(training_data_len_extracted[i]),3]=training_data_extracted[i,:int(training_data_len_extracted[i]),3]\n",
    "\n",
    "\n",
    "        if index==1:\n",
    "            training_data_rotated[i,:int(training_data_len_extracted[i]),3]=training_data_extracted[i,:int(training_data_len_extracted[i]),3]\n",
    "            training_data_rotated[i,:int(training_data_len_extracted[i]),2]=training_data_extracted[i,:int(training_data_len_extracted[i]),2]\n",
    "            training_data_rotated[i,:int(training_data_len_extracted[i]),1]=training_data_extracted[i,:int(training_data_len_extracted[i]),1]\n",
    "\n",
    "\n",
    "        if index==2:\n",
    "            training_data_rotated[i,:int(training_data_len_extracted[i]),2]=training_data_extracted[i,:int(training_data_len_extracted[i]),3]\n",
    "            training_data_rotated[i,:int(training_data_len_extracted[i]),3]=training_data_extracted[i,:int(training_data_len_extracted[i]),2]\n",
    "            training_data_rotated[i,:int(training_data_len_extracted[i]),1]=training_data_extracted[i,:int(training_data_len_extracted[i]),1]\n",
    "    \n",
    "    if mean_y_sign<0:\n",
    "\n",
    "        if index==0:\n",
    "            training_data_rotated[i,:int(training_data_len_extracted[i]),2]=-training_data_extracted[i,:int(training_data_len_extracted[i]),1]\n",
    "            training_data_rotated[i,:int(training_data_len_extracted[i]),1]=training_data_extracted[i,:int(training_data_len_extracted[i]),2]\n",
    "            training_data_rotated[i,:int(training_data_len_extracted[i]),3]=training_data_extracted[i,:int(training_data_len_extracted[i]),3]\n",
    "\n",
    "\n",
    "        if index==1:\n",
    "            training_data_rotated[i,:int(training_data_len_extracted[i]),3]=training_data_extracted[i,:int(training_data_len_extracted[i]),3]\n",
    "            training_data_rotated[i,:int(training_data_len_extracted[i]),2]=-training_data_extracted[i,:int(training_data_len_extracted[i]),2]\n",
    "            training_data_rotated[i,:int(training_data_len_extracted[i]),1]=training_data_extracted[i,:int(training_data_len_extracted[i]),1]\n",
    "\n",
    "\n",
    "        if index==2:\n",
    "            training_data_rotated[i,:int(training_data_len_extracted[i]),2]=-training_data_extracted[i,:int(training_data_len_extracted[i]),3]\n",
    "            training_data_rotated[i,:int(training_data_len_extracted[i]),3]=training_data_extracted[i,:int(training_data_len_extracted[i]),2]\n",
    "            training_data_rotated[i,:int(training_data_len_extracted[i]),1]=training_data_extracted[i,:int(training_data_len_extracted[i]),1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segmentation\n",
    "#Again filter with low cut of frequency used to get a clean sinusoidal oscillation to find the relevant peaks easily\n",
    "\n",
    "\n",
    "cutFreq = 1.8\n",
    "training_data_filtered_3=np.zeros((number_of_inputs,60000,4))\n",
    "training_data_filtered_3[:,:,0]=training_data_rotated[:,:,0]\n",
    "\"A single Butterworth filter with a cutoff frequency between 0.1 and 15 Hz is \"\n",
    "\"sufficient to reduce a large component of noise from the data and serves \"\n",
    "\"as the best preprocessing step\"\n",
    "#     sampling_rate = round(3/(time[2]-time[1]),3)    # sampling rate 416 HZ \n",
    "for i in range(number_of_inputs):\n",
    "    for j in range(1,4):\n",
    "        sampling_rate=training_data_freq[i]\n",
    "        if sampling_rate >= 3.6:\n",
    "            cutoff_freq = cutFreq/(0.5*sampling_rate)  \n",
    "            b,a = butter(2, cutoff_freq, btype = 'low',analog= False)\n",
    "            data = training_data_rotated[i,:int(training_data_len_extracted[i]-1),j] #dataset.iloc[:,1:2]\n",
    "            data = np.transpose(data)\n",
    "            filtered_data = filtfilt(b,a,data)\n",
    "            training_data_filtered_3[i,:int(training_data_len_extracted[i]-1),j] = np.transpose(filtered_data)\n",
    "\n",
    "print(training_data_filtered_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The walking sequence gets cut into individual steps based on the peaks of the signal\n",
    "\n",
    "\n",
    "training_data_sampled=np.zeros((number_of_inputs,190,1400,4)) ##number of input, number of samples, sampled data, axis\n",
    "\n",
    "peaks_i=np.zeros((number_of_inputs,700))\n",
    "num_of_peaks=np.zeros((number_of_inputs))\n",
    "peak_distance_i=np.zeros((number_of_inputs,700))\n",
    "\n",
    "# for i in range(30,40):\n",
    "for i in range(number_of_inputs):\n",
    "     \n",
    "    peak_indices_1, _ =find_peaks(training_data_filtered_3[i,0:int(training_data_len_extracted[i]),2], prominence=0.5)#2.8 prom 5,distance=150 #,distance=training_data_freq[1]*0.5\n",
    "\n",
    "    peak_indices=peak_indices_1\n",
    "    peak_distance=np.zeros(len(peak_indices))\n",
    "    peaks_i[i,:len(peak_indices)]=peak_indices\n",
    "    num_of_peaks[i]=len(peak_indices)\n",
    "    \n",
    "    for j in range(len(peak_indices)-1):#\n",
    "\n",
    "        peak_distance[j]=peak_indices[j+1]-peak_indices[j]\n",
    "        \n",
    "        peak_distance_i[i,j]=peak_distance[j]\n",
    "        \n",
    "        for k in range (4):\n",
    "            training_data_sampled[i,j,:int(peak_distance[j]),k]=training_data_rotated[i,peak_indices[j]:peak_indices[j+1],k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show Plot of segmented data but with different sample length\n",
    "sample_length=500\n",
    "for i in range(1): #(number_of_imputs):\n",
    "# for i in range(number_of_imputs):\n",
    "\n",
    "    for j in range(int(num_of_peaks[i])-1):#\n",
    "        \n",
    "        if training_label[i]==0:\n",
    "            plt.figure(1)\n",
    "            plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "            plt.plot(np.linspace(0,sample_length,sample_length),training_data_sampled[i,j,0:sample_length,1])\n",
    "            plt.figure(2)\n",
    "            plt.plot(np.linspace(0,sample_length,sample_length),training_data_sampled[i,j,0:sample_length,2])\n",
    "            plt.figure(3)\n",
    "            plt.plot(np.linspace(0,sample_length,sample_length),training_data_sampled[i,j,0:sample_length,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample \n",
    "#Every step gets resampled to the same number of datapoints\n",
    "#Two peaks from the segmentation make up one step and the order of the reconstruction gets adressed based on the maximum value. th\n",
    "\n",
    "resample_length=300\n",
    "training_data_resampled=np.zeros((number_of_inputs,int(np.max(num_of_peaks)//2-1),resample_length,3))\n",
    "\n",
    "# for i in range(40):\n",
    "for i in range(number_of_inputs):\n",
    "    max_even=0\n",
    "    max_odd=0\n",
    "    for j in range((int(num_of_peaks[i]//2)-1)):#\n",
    "        max_even+=np.max(training_data_sampled[i,2*j,:int(peak_distance_i[i,2*j])//2,2])\n",
    "        max_odd+=np.max(training_data_sampled[i,2*j+1,:int(peak_distance_i[i,2*j+1])//2,2])\n",
    "                    \n",
    "    mean_max_even=max_even\n",
    "    mean_max_odd=max_odd\n",
    "\n",
    "    if mean_max_even>mean_max_odd:\n",
    "        for j in range((int(num_of_peaks[i]//2)-1)):#\n",
    "            for k in range(3):#\n",
    "                x1=training_data_sampled[i,2*j,:int(peak_distance_i[i,2*j]),k+1]\n",
    "                x2=training_data_sampled[i,2*j+1,:int(peak_distance_i[i,2*j+1]),k+1]\n",
    "                x3=np.concatenate((x1,x2))\n",
    "                #training_data_resampled[i,j,:,0]=\n",
    "                training_data_resampled[i,j,:,k]=signal.resample(x3,resample_length)\n",
    "\n",
    "    else:\n",
    "        for j in range((int(num_of_peaks[i]//2)-2)):#                                                                                   \n",
    "            for k in range(3):#\n",
    "                x1=training_data_sampled[i,2*j+1,:int(peak_distance_i[i,2*j+1]),k+1]\n",
    "                x2=training_data_sampled[i,2*j+2,:int(peak_distance_i[i,2*j+2]),k+1]\n",
    "                x3=np.concatenate((x1,x2))\n",
    "                #training_data_resampled[i,j,:,0]=\n",
    "                training_data_resampled[i,j,:,k]=signal.resample(x3,resample_length)\n",
    "            #   print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for the resampled data\n",
    "\n",
    "for i in range(0,6): #(number_of_imputs):\n",
    "# for i in range(number_of_imputs):\n",
    "\n",
    "    for j in range(0,int(num_of_peaks[i])//2-1):#\n",
    "        \n",
    "        if training_label[i]==0:\n",
    "#         if (j % 2) == 0:\n",
    "            plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "#             plt.figure(3*i)\n",
    "#             plt.title(str(i)+\" subj: \"+str(training_subject[i]))\n",
    "#             plt.plot(np.linspace(0,resample_length,resample_length),training_data_resampled[i,j,0:resample_length,0])\n",
    "            \n",
    "            plt.figure(3*i+1)\n",
    "            plt.title(str(i)+\" subj: \"+str(training_subject[i]))\n",
    "            plt.plot(np.linspace(0,resample_length,resample_length),training_data_resampled[i,j,0:resample_length,1])\n",
    "#             plt.figure(3*i+2)\n",
    "#             plt.plot(np.linspace(0,resample_length,resample_length),training_data_resampled[i,j,0:resample_length,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning of Data\n",
    "#The mean of all the data points across all steps were taken and if the mean of the data points \n",
    "#differed too much then that step was removed. \n",
    "\n",
    "training_data_cleaned=np.zeros((number_of_inputs,int(np.max(num_of_peaks)//2-1),resample_length,3))\n",
    "# standart_deviation=np.zeros(int(np.max(num_of_peaks)//2-1))\n",
    "# mean_of_sample_point=np.zeros(resample_length)\n",
    "deviation_of_mean=np.zeros((int(np.max(num_of_peaks)//2-1),resample_length))\n",
    "average_deviation=np.zeros(int(np.max(num_of_peaks)//2-1))\n",
    "\n",
    "# for j in range(0,int(num_of_peaks[i])//2-1):#\n",
    "# for i in range(330,334): #(number_of_inputs):\n",
    "for i in range(number_of_inputs): \n",
    "    for l in range(resample_length):#\n",
    "        \n",
    "            mean_of_sample_point=np.mean(training_data_resampled[i,training_data_resampled[i,:,l,1].nonzero(),l,1]) \n",
    "            \n",
    "            for j in range(0,int(num_of_peaks[i])//2-1):#\n",
    "                \n",
    "                deviation_of_mean[j,l]=(training_data_resampled[i,j,l,1]-mean_of_sample_point)**2\n",
    "       \n",
    "    for j in range(0,int(num_of_peaks[i])//2-1):#\n",
    "            \n",
    "        average_deviation[j]=np.mean(deviation_of_mean[j,deviation_of_mean[j,:].nonzero()])   \n",
    "        if average_deviation[j]>1.8:\n",
    "            training_data_cleaned[i,j,:,:]=0\n",
    "        else:                            \n",
    "            training_data_cleaned[i,j,:,:]=training_data_resampled[i,j,:,:]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of cleaned data\n",
    "for i in range(0,18): #(number_of_imputs):\n",
    "# for i in range(number_of_imputs):\n",
    "\n",
    "    for j in range(0,int(num_of_peaks[i])//2-1):#\n",
    "        \n",
    "        if training_label[i]==0:\n",
    "#         if (j % 2) == 0:\n",
    "            plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "#             plt.figure(3*i)\n",
    "#             plt.title(str(i)+\" subj: \"+str(training_subject[i]))\n",
    "#             plt.plot(np.linspace(0,resample_length,resample_length),training_data_cleaned[i,j,0:resample_length,0])\n",
    "            \n",
    "            plt.figure(3*i+1)\n",
    "            plt.title(str(i)+\" subj: \"+str(training_subject[i]))\n",
    "            plt.plot(np.linspace(0,resample_length,resample_length),training_data_cleaned[i,j,0:resample_length,1])\n",
    "#             plt.figure(3*i+2)\n",
    "#             plt.plot(np.linspace(0,resample_length,resample_length),training_data_cleaned[i,j,0:resample_length,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct Training Input Array\n",
    "#Only the valid data gets taken and assemled into one large array as the input for the model\n",
    "#One input of the data contains one step and all axis as a flattened array\n",
    "\n",
    "training_input_zeros=np.zeros((60000,3*resample_length))\n",
    "training_input_label_zeros=np.zeros((60000))\n",
    "                        \n",
    "k=0\n",
    "for i in range(number_of_inputs):\n",
    "# for i in range(1):\n",
    "    for j in range(int(num_of_peaks[i])//2-1):#\n",
    "        \n",
    "        if np.count_nonzero(training_data_cleaned[i,j,:,1])!=0:\n",
    "#         nonzero_training_data_cleaned=training_data_cleaned[i,j,:,:]\n",
    "        \n",
    "            training_input_zeros[k,:300]=training_data_cleaned[i,j,:,0]\n",
    "            training_input_zeros[k,300:600]=training_data_cleaned[i,j,:,1]\n",
    "            training_input_zeros[k,600:900]=training_data_cleaned[i,j,:,2]\n",
    "            training_input_label_zeros[k]=training_label[i]     \n",
    "                                       \n",
    "            k+=1   \n",
    "            \n",
    "training_input=np.zeros((k,3*resample_length))   \n",
    "training_input_label=np.zeros((k))\n",
    "\n",
    "training_input[:,:]=training_input_zeros[:k,:]\n",
    "training_input_label[:]=training_input_label_zeros[:k]\n",
    "\n",
    "print(training_input.shape)\n",
    "print(training_input_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of the input for the network\n",
    "\n",
    "plt.figure(1)\n",
    "                 \n",
    "# for i in range (training_input.shape[0]):\n",
    "for i in range (0,25):\n",
    "#     plt.title(str(i)+\" subj: \"+str(training_subject[i]))\n",
    "    plt.plot(np.linspace(0,resample_length*3,resample_length*3),training_input[i,:])\n",
    "#     plt.plot(np.linspace(0,100,100),training_input[i,:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_input\n",
    "y = training_input_label\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(X)\n",
    "#print(kf)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    #y_pred = forest.predict(X_test[:])\n",
    "    #print('Test Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(900,)), \n",
    "        tf.keras.layers.Dense(1000, activation ='relu'),\n",
    "        tf.keras.layers.Dense(500, activation ='relu'),\n",
    "        tf.keras.layers.Dense(3, activation ='softmax')\n",
    "        ])\n",
    "        \n",
    "    model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    # train the neural network for 5 training epochs\n",
    "    history = model.fit(X_train, y_train, epochs = 50, validation_data = (X_test, y_test), verbose = 1);\n",
    "        \n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1);\n",
    "\n",
    "    print('Loss=', loss)\n",
    "    print('accuracy=', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learningCurve(history, epochs):\n",
    "    #Plot training & validation accuracy values\n",
    "    epoch_range = range(1, epochs+1)\n",
    "    plt.plot(epoch_range, history.history['accuracy'])\n",
    "    plt.plot(epoch_range, history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc = 'upper left')\n",
    "    plt.show()\n",
    "\n",
    "    #Plot training & validation loss values\n",
    "    plt.plot(epoch_range, history.history['loss'])\n",
    "    plt.plot(epoch_range, history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc = 'upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learningCurve(history, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "classes_x = np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y_test, classes_x)\n",
    "plot_confusion_matrix(conf_mat=mat, class_names=label.classes_, show_normed=False, figsize = (10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_d = np.zeros((800,60000,4))\n",
    "print(t_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d46ac241e10bc7ad90ee82a7fa74784bf955a2c7ba6b7b73b8ebd330188e3a2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
